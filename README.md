# Cluster Analysis Using Unsupervised Learning  
In this project I have created 3 tasks:
1. Build KMeans cluster using the original Fashion-MNIST dataset.
2. Build an Auto-Encoder based K-Means clustering model to cluster the condensed
representation of the unlabeled fashion MNIST dataset using Keras and Sklearns library.
3. Build an Auto-Encoder based Gaussian Mixture Model clustering model to cluster the condensed representation of the unlabeled fashion MNIST dataset using Keras and Sklearns library.
Furthermore, report the accuracy for each task.
The images are a set of 28 X 28 pixels, i.e. 784 total pixels. For this project we need to perform KMeans clustering on the dataset. After performing KMeans clustering on the original dataset we need to use Autoencoders to compress and decompress an image, where we take the input image and recreate the input image at the output as an output image. After implementing KMeans we need to implement the Gaussian Mixture Model (GMM). This is a probabilistic model. It uses probability of a sample to determine the feasibility of it belonging to a cluster. These models are implemented in the project.
### Task 1: 
In this task we have to find the KMeans clustering accuracy on the original images. For this task we use the inbuilt library for KMeans clustering, sklearn.cluster from which we import the KMeans clustering algorithm. We then import the Fashion MNIST dataset and pre-process the data. After pre-processing the data we feed the training data to the KMeans clustering model imported form sklearn library. We set the number of clusters as 10 as we know there are 10 classes in this dataset. However, if we do not know the number of classes for the data then we can use the elbow method to determine the optimal value for the total number of clusters to be created. After implementing the KMeans model we perform the fit method on the training data and run the predictions on the testing data. Later we find the accuracy of both the training and testing data. We also build a confusion matrix for the testing data. We can perform on either data, training or testing or both, because as far as the clustering algorithms go, they are unsupervised and hence donâ€™t have the ground truth to perform any accuracy metrics, but here we have the respective test and train data on which we perform the accuracy calculations. Further we also build the actual loss and the validation loss graphs against the training epochs.
### Task 2: 
For the second task we have to build autoencoder. The autoencoders are compression algorithms. For this project we use autoencoders because as per the requirements we have to cluster the data using the latent feature space which can be obtained after the encoder model is created. The basic working of an autoencoder is that we have to generate the input image as an output image with reduced or no noise. The encoder in this algorithm compresses or reduces the dimensions of the original image and the decoders revert it back to the original dimensions. Here in the project we have created 3 encoder layers and 3 decoder layers. The encoder layers have the dimensions in a decreasing fashion whereas the decoders have dimensions in increasing fashion to retrieve the image in its original form. After that we extract the encoder layers and create an encoder model to get only the encoded images which as per the requirement are to be fed for the predictions and then compiling and building the autoencoder model is done. Further we find the accuracy and build a confusion matrix as explained in the first task.
### Task 3: 
This task is almost similar to the second task with the only difference of the model used. Here in this task we use the Gaussian Mixture Model (GMM). This model is a probabilistic which allows us to make soft predictions unlike the KMeans model. It chooses the clusters depending the probability results. The GMM model is directly imported sklearn.mixture library as GaussianMixture. The GMM also has the similar functions of fit and predict which are used to make predictions for the given data set. Similar to the above to tasks we perform accuracy calculation and build a confusion matrix for this model.
### Dataset Definition
In this project we are dealing with Fashion MNIST dataset. This dataset consists 70,000 images of size 28 X 28. The dataset is split into training and testing sets in the ratio 6:1. There are a total of 784 pixels per image associated with a grey Cale value to each pixel. The pixel value ranges from 0 to 255. This dataset has 10 classes and each of the image is associated with one of the 10 classes.
The 10 classes are:
T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle Boot.
